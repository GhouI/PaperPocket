import { LlamaContext } from './index';
import type { ContextParams, CompletionParams, CactusOAICompatibleMessage, NativeCompletionResult, EmbeddingParams, NativeEmbeddingResult } from './index';
import { ConversationHistoryManager } from './chat';
interface CactusLMReturn {
    lm: CactusLM | null;
    error: Error | null;
}
export declare class CactusLM {
    protected context: LlamaContext;
    protected conversationHistoryManager: ConversationHistoryManager;
    private initParams;
    private static _initCache;
    private static getCacheKey;
    protected constructor(context: LlamaContext, initParams: ContextParams);
    private static isContextNotFoundError;
    private reinit;
    private run;
    static init(params: ContextParams, onProgress?: (progress: number) => void, cactusToken?: string, retryOptions?: {
        maxRetries?: number;
        delayMs?: number;
    }): Promise<CactusLMReturn>;
    completion: (messages: CactusOAICompatibleMessage[], params?: CompletionParams & {
        mode?: string;
    }, callback?: (data: any) => void) => Promise<NativeCompletionResult>;
    private _handleLocalCompletion;
    private _handleRemoteCompletion;
    embedding(text: string, params?: EmbeddingParams, mode?: string): Promise<NativeEmbeddingResult>;
    protected _handleLocalEmbedding(text: string, params?: EmbeddingParams): Promise<NativeEmbeddingResult>;
    protected _handleRemoteEmbedding(text: string): Promise<NativeEmbeddingResult>;
    rewind: () => Promise<void>;
    release(): Promise<void>;
    stopCompletion(): Promise<void>;
    isJinjaSupported(): boolean;
}
export {};
//# sourceMappingURL=lm.d.ts.map