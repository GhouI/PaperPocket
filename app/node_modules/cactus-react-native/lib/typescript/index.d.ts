import type { NativeContextParams, NativeLlamaContext, NativeCompletionParams, NativeCompletionTokenProb, NativeCompletionResult, NativeTokenizeResult, NativeEmbeddingResult, NativeSessionLoadResult, NativeEmbeddingParams, NativeCompletionTokenProbItem, NativeCompletionResultTimings, JinjaFormattedChatResult, NativeTTSType, NativeAudioCompletionResult, NativeAudioTokensResult, NativeAudioDecodeResult, NativeDeviceInfo } from './NativeCactus';
import type { CactusMessagePart, CactusOAICompatibleMessage } from './chat';
import { Tools } from './tools';
export type { NativeContextParams, NativeLlamaContext, NativeCompletionParams, NativeCompletionTokenProb, NativeCompletionResult, NativeTokenizeResult, NativeEmbeddingResult, NativeSessionLoadResult, NativeEmbeddingParams, NativeCompletionTokenProbItem, NativeCompletionResultTimings, CactusMessagePart, CactusOAICompatibleMessage, JinjaFormattedChatResult, NativeAudioDecodeResult, NativeTTSType, };
export * from './remote';
export { Tools, parseAndExecuteTool } from './tools';
export type TokenData = {
    token: string;
    completion_probabilities?: Array<NativeCompletionTokenProb>;
};
export type ContextParams = Omit<NativeContextParams, 'cache_type_k' | 'cache_type_v' | 'pooling_type'> & {
    cache_type_k?: 'f16' | 'f32' | 'q8_0' | 'q4_0' | 'q4_1' | 'iq4_nl' | 'q5_0' | 'q5_1';
    cache_type_v?: 'f16' | 'f32' | 'q8_0' | 'q4_0' | 'q4_1' | 'iq4_nl' | 'q5_0' | 'q5_1';
    pooling_type?: 'none' | 'mean' | 'cls' | 'last' | 'rank';
};
export type EmbeddingParams = NativeEmbeddingParams;
export type CompletionResponseFormat = {
    type: 'text' | 'json_object' | 'json_schema';
    json_schema?: {
        strict?: boolean;
        schema: object;
    };
    schema?: object;
};
export type CompletionBaseParams = {
    prompt?: string;
    messages?: CactusOAICompatibleMessage[];
    chatTemplate?: string;
    chat_template?: string;
    jinja?: boolean;
    tools?: object;
    parallel_tool_calls?: object;
    tool_choice?: string;
    response_format?: CompletionResponseFormat;
};
export type CompletionParams = Omit<NativeCompletionParams, 'emit_partial_completion' | 'prompt'> & CompletionBaseParams;
export type BenchResult = {
    modelDesc: string;
    modelSize: number;
    modelNParams: number;
    ppAvg: number;
    ppStd: number;
    tgAvg: number;
    tgStd: number;
};
export declare class LlamaContext {
    id: number;
    gpu: boolean;
    reasonNoGPU: string;
    model: NativeLlamaContext['model'];
    constructor({ contextId, gpu, reasonNoGPU, model }: NativeLlamaContext);
    loadSession(filepath: string): Promise<NativeSessionLoadResult>;
    saveSession(filepath: string, options?: {
        tokenSize: number;
    }): Promise<number>;
    isLlamaChatSupported(): boolean;
    isJinjaSupported(): boolean;
    getFormattedChat(messages: CactusOAICompatibleMessage[], template?: string | null, params?: {
        jinja?: boolean;
        response_format?: CompletionResponseFormat;
        tools?: object;
        parallel_tool_calls?: object;
        tool_choice?: string;
    }): Promise<JinjaFormattedChatResult | string>;
    completionWithTools(params: CompletionParams & {
        tools: Tools;
    }, callback?: (data: TokenData) => void, recursionCount?: number, recursionLimit?: number): Promise<NativeCompletionResult>;
    completion(params: CompletionParams, callback?: (data: TokenData) => void): Promise<NativeCompletionResult>;
    stopCompletion(): Promise<void>;
    tokenize(text: string): Promise<NativeTokenizeResult>;
    detokenize(tokens: number[]): Promise<string>;
    embedding(text: string, params?: EmbeddingParams): Promise<NativeEmbeddingResult>;
    bench(pp: number, tg: number, pl: number, nr: number): Promise<BenchResult>;
    applyLoraAdapters(loraList: Array<{
        path: string;
        scaled?: number;
    }>): Promise<void>;
    removeLoraAdapters(): Promise<void>;
    getLoadedLoraAdapters(): Promise<Array<{
        path: string;
        scaled?: number;
    }>>;
    release(): Promise<void>;
    rewind(): Promise<void>;
}
export declare function toggleNativeLog(enabled: boolean): Promise<void>;
export declare function addNativeLogListener(listener: (level: string, text: string) => void): {
    remove: () => void;
};
export declare function setContextLimit(limit: number): Promise<void>;
export declare function loadLlamaModelInfo(model: string): Promise<Object>;
export declare function initLlama({ model, is_model_asset: isModelAsset, pooling_type: poolingType, lora, lora_list: loraList, ...rest }: ContextParams, onProgress?: (progress: number) => void): Promise<LlamaContext>;
export declare function releaseAllLlama(): Promise<void>;
export declare const initContext: (params: NativeContextParams) => Promise<NativeLlamaContext>;
export declare const initMultimodal: (contextId: number, mmprojPath: string, useGpu?: boolean) => Promise<boolean>;
export declare const isMultimodalEnabled: (contextId: number) => Promise<boolean>;
export declare const isMultimodalSupportVision: (contextId: number) => Promise<boolean>;
export declare const isMultimodalSupportAudio: (contextId: number) => Promise<boolean>;
export declare const releaseMultimodal: (contextId: number) => Promise<void>;
export declare const multimodalCompletion: (contextId: number, prompt: string, mediaPaths: string[], params: NativeCompletionParams) => Promise<NativeCompletionResult>;
export declare const initVocoder: (contextId: number, vocoderModelPath: string) => Promise<boolean>;
export declare const isVocoderEnabled: (contextId: number) => Promise<boolean>;
export declare const getTTSType: (contextId: number) => Promise<NativeTTSType>;
export declare const getFormattedAudioCompletion: (contextId: number, speakerJsonStr: string, textToSpeak: string) => Promise<NativeAudioCompletionResult>;
export declare const getAudioCompletionGuideTokens: (contextId: number, textToSpeak: string) => Promise<NativeAudioTokensResult>;
export declare const decodeAudioTokens: (contextId: number, tokens: number[]) => Promise<NativeAudioDecodeResult>;
export declare const releaseVocoder: (contextId: number) => Promise<void>;
export declare const tokenize: (contextId: number, text: string, mediaPaths?: string[]) => Promise<NativeTokenizeResult>;
export declare const getDeviceInfo: (contextId: number) => Promise<NativeDeviceInfo>;
export { CactusLM } from './lm';
export { CactusVLM } from './vlm';
export { CactusTTS } from './tts';
export { CactusAgent } from './agent';
//# sourceMappingURL=index.d.ts.map