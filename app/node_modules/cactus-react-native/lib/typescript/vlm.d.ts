import { type ContextParams, type CompletionParams, type CactusOAICompatibleMessage, type NativeCompletionResult } from './index';
import { ConversationHistoryManager } from './chat';
interface CactusVLMReturn {
    vlm: CactusVLM | null;
    error: Error | null;
}
export type VLMContextParams = ContextParams & {
    mmproj: string;
};
export type VLMCompletionParams = Omit<CompletionParams, 'prompt'> & {
    images?: string[];
    mode?: string;
};
export declare class CactusVLM {
    private context;
    protected conversationHistoryManager: ConversationHistoryManager;
    private static _initCache;
    private static getCacheKey;
    private constructor();
    static init(params: VLMContextParams, onProgress?: (progress: number) => void, cactusToken?: string, retryOptions?: {
        maxRetries?: number;
        delayMs?: number;
    }): Promise<CactusVLMReturn>;
    completion(messages: CactusOAICompatibleMessage[], params?: VLMCompletionParams, callback?: (data: any) => void): Promise<NativeCompletionResult>;
    private _handleLocalCompletion;
    private _handleRemoteCompletion;
    rewind(): Promise<void>;
    release(): Promise<void>;
    stopCompletion(): Promise<void>;
    isJinjaSupported(): boolean;
}
export {};
//# sourceMappingURL=vlm.d.ts.map