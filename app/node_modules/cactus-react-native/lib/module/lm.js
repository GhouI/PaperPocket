"use strict";

import { initLlama } from './index';
// @ts-ignore

import { Telemetry } from './telemetry';
import { setCactusToken, getVertexAIEmbedding, getTextCompletion } from './remote';
import { ConversationHistoryManager } from './chat';
export class CactusLM {
  static _initCache = new Map();
  static getCacheKey(params, cactusToken, retryOptions) {
    return JSON.stringify({
      params,
      cactusToken,
      retryOptions
    });
  }
  constructor(context, initParams) {
    this.context = context;
    this.initParams = initParams;
    this.conversationHistoryManager = new ConversationHistoryManager();
  }
  static isContextNotFoundError(e) {
    const message = String(e?.message ?? e ?? '');
    return /context not found/i.test(message);
  }
  async reinit() {
    const newContext = await initLlama(this.initParams);
    this.context = newContext;
    this.conversationHistoryManager.reset();
  }
  async run(op) {
    try {
      return await op();
    } catch (e) {
      if (!CactusLM.isContextNotFoundError(e)) throw e;
      await this.reinit();
      return await op();
    }
  }
  static async init(params, onProgress, cactusToken, retryOptions) {
    if (cactusToken) {
      setCactusToken(cactusToken);
    }
    const key = CactusLM.getCacheKey(params, cactusToken, retryOptions);
    if (CactusLM._initCache.has(key)) {
      return CactusLM._initCache.get(key);
    }
    const initPromise = (async () => {
      const maxRetries = retryOptions?.maxRetries ?? 3;
      const delayMs = retryOptions?.delayMs ?? 1000;
      const configs = [params, {
        ...params,
        n_gpu_layers: 0
      }];
      const sleep = ms => {
        return new Promise(resolve => {
          const start = Date.now();
          const wait = () => {
            if (Date.now() - start >= ms) {
              resolve();
            } else {
              Promise.resolve().then(wait);
            }
          };
          wait();
        });
      };
      for (const config of configs) {
        let lastError = null;
        for (let attempt = 1; attempt <= maxRetries; attempt++) {
          try {
            const context = await initLlama(config, onProgress);
            return {
              lm: new CactusLM(context, config),
              error: null
            };
          } catch (e) {
            lastError = e;
            const isLastConfig = configs.indexOf(config) === configs.length - 1;
            const isLastAttempt = attempt === maxRetries;
            Telemetry.error(e, {
              n_gpu_layers: config.n_gpu_layers ?? null,
              n_ctx: config.n_ctx ?? null,
              model: config.model ?? null
            });
            if (!isLastAttempt) {
              const delay = delayMs * Math.pow(2, attempt - 1);
              await sleep(delay);
            } else if (!isLastConfig) {
              break;
            }
          }
        }
        if (configs.indexOf(config) === configs.length - 1 && lastError) {
          return {
            lm: null,
            error: lastError
          };
        }
      }
      return {
        lm: null,
        error: new Error('Failed to initialize CactusLM after all retries')
      };
    })();
    CactusLM._initCache.set(key, initPromise);
    const result = await initPromise;
    // Cache only while in-flight; never cache resolved instances
    CactusLM._initCache.delete(key);
    return result;
  }
  completion = async (messages, params = {}, callback) => {
    const mode = params.mode || 'local';
    let result;
    let lastError = null;
    if (mode === 'remote') {
      result = await this._handleRemoteCompletion(messages, callback);
    } else if (mode === 'local') {
      result = await this._handleLocalCompletion(messages, params, callback);
    } else if (mode === 'localfirst') {
      try {
        result = await this._handleLocalCompletion(messages, params, callback);
      } catch (e) {
        lastError = e;
        try {
          result = await this._handleRemoteCompletion(messages, callback);
        } catch (remoteError) {
          throw lastError;
        }
      }
    } else if (mode === 'remotefirst') {
      try {
        result = await this._handleRemoteCompletion(messages, callback);
      } catch (e) {
        lastError = e;
        try {
          result = await this._handleLocalCompletion(messages, params, callback);
        } catch (localError) {
          throw lastError;
        }
      }
    } else {
      throw new Error('Invalid mode: ' + mode + '. Must be "local", "remote", "localfirst", or "remotefirst"');
    }
    return result;
  };
  _handleLocalCompletion = async (messages, params, callback) => {
    const {
      newMessages,
      requiresReset
    } = this.conversationHistoryManager.processNewMessages(messages);
    if (requiresReset) {
      await this.run(() => this.context.rewind());
      this.conversationHistoryManager.reset();
    }
    if (newMessages.length === 0) {
      console.warn('No messages to complete!');
    }
    const result = await this.run(() => this.context.completion({
      messages: newMessages,
      ...params
    }, callback));
    this.conversationHistoryManager.update(newMessages, {
      role: 'assistant',
      content: result.content
    });
    return result;
  };
  async _handleRemoteCompletion(messages, callback) {
    const prompt = messages.map(m => `${m.role}: ${m.content}`).join('\n');
    const responseText = await getTextCompletion(messages);
    if (callback) {
      for (let i = 0; i < responseText.length; i++) {
        callback({
          token: responseText[i]
        });
      }
    }
    return {
      text: responseText,
      reasoning_content: '',
      tool_calls: [],
      content: responseText,
      tokens_predicted: responseText.split(' ').length,
      tokens_evaluated: prompt.split(' ').length,
      truncated: false,
      stopped_eos: true,
      stopped_word: '',
      stopped_limit: 0,
      stopping_word: '',
      tokens_cached: 0,
      timings: {
        prompt_n: prompt.split(' ').length,
        prompt_ms: 0,
        prompt_per_token_ms: 0,
        prompt_per_second: 0,
        predicted_n: responseText.split(' ').length,
        predicted_ms: 0,
        predicted_per_token_ms: 0,
        predicted_per_second: 0
      }
    };
  }
  async embedding(text, params, mode = 'local') {
    let result;
    let lastError = null;
    if (mode === 'remote') {
      result = await this._handleRemoteEmbedding(text);
    } else if (mode === 'local') {
      result = await this._handleLocalEmbedding(text, params);
    } else if (mode === 'localfirst') {
      try {
        result = await this._handleLocalEmbedding(text, params);
      } catch (e) {
        lastError = e;
        try {
          result = await this._handleRemoteEmbedding(text);
        } catch (remoteError) {
          throw lastError;
        }
      }
    } else if (mode === 'remotefirst') {
      try {
        result = await this._handleRemoteEmbedding(text);
      } catch (e) {
        lastError = e;
        try {
          result = await this._handleLocalEmbedding(text, params);
        } catch (localError) {
          throw lastError;
        }
      }
    } else {
      throw new Error('Invalid mode: ' + mode + '. Must be "local", "remote", "localfirst", or "remotefirst"');
    }
    return result;
  }
  async _handleLocalEmbedding(text, params) {
    return this.run(() => this.context.embedding(text, params));
  }
  async _handleRemoteEmbedding(text) {
    const embeddingValues = await getVertexAIEmbedding(text);
    return {
      embedding: embeddingValues
    };
  }
  rewind = async () => {
    return this.run(() => this.context.rewind());
  };
  async release() {
    try {
      return await this.context.release();
    } catch (e) {
      // Treat missing context as already released
      if (CactusLM.isContextNotFoundError(e)) return;
      throw e;
    }
  }
  async stopCompletion() {
    return await this.run(() => this.context.stopCompletion());
  }
  isJinjaSupported() {
    return this.context.isJinjaSupported();
  }
}
//# sourceMappingURL=lm.js.map